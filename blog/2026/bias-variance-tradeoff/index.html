<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Monish  Keswani


  | Bias-Variance Tradeoff: From Theory to Ensemble Methods

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->

<meta property="og:site_name" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="https://archmaester.github.io/blog/2026/bias-variance-tradeoff/" />
<meta property="og:description" content="Bias-Variance Tradeoff: From Theory to Ensemble Methods" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí°</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2026/bias-variance-tradeoff/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>







<!-- GoatCounter Analytics -->
<script data-goatcounter="https://archmaester.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://archmaester.github.io/">
       <span class="font-weight-bold">Monish</span>   Keswani
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/contact/">
                contact
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/cv/">
                cv
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <a href="/blog/" class="back-to-blog">&larr; Back to blog</a>

  <header class="post-header">
    <h1 class="post-title">Bias-Variance Tradeoff: From Theory to Ensemble Methods</h1>
    
    
    
    <p class="post-meta">
      February 15, 2026
      
      <span class="reading-time">&middot; 5 min read</span>
    </p>
    
    <p class="post-description-text">A deep dive into the bias-variance decomposition, decision trees, bagging, boosting, and XGBoost ‚Äî with clear math and intuition.</p>
    
    
    <div class="post-tags">
      
      <span class="tag-badge">machine-learning</span>
      
      <span class="tag-badge">ensemble-methods</span>
      
      <span class="tag-badge">decision-trees</span>
      
    </div>
    
  </header>

  
  
  
  
  
  
  <nav class="toc-wrapper">
    <div class="toc-title">Table of Contents</div>
    <ul>
      
        
        
          <li><a href="#1-the-core-problem-bias-vs-variance">1. The Core Problem: Bias vs. Variance</a></li>
        
      
        
        
          <li><a href="#2-the-building-block-decision-trees">2. The Building Block: Decision Trees</a></li>
        
      
        
        
          <li><a href="#3-the-ensemble-methods-bagging--boosting">3. The Ensemble Methods: Bagging &amp; Boosting</a></li>
        
      
        
        
          <li><a href="#4-the-advanced-algorithms">4. The Advanced Algorithms</a></li>
        
      
    </ul>
  </nav>
  

  <article class="post-content">
    <h2 id="1-the-core-problem-bias-vs-variance">1. The Core Problem: Bias vs. Variance</h2>

<p>At the heart of machine learning is the <strong>Bias-Variance Tradeoff</strong>. This is the challenge of creating a model that is both accurate and reliable.</p>

<ul>
  <li><strong>Bias:</strong> This is the error from your model being <em>too simple</em>. A high-bias model ‚Äúunderfits‚Äù the data. It makes strong assumptions that aren‚Äôt true, so it‚Äôs inaccurate on <em>both</em> the data it was trained on and new data.</li>
  <li>
    <p><strong>Analogy:</strong> An archer whose bow is misaligned. All their shots consistently hit the same wrong spot (e.g., low and to the left). The model is consistently wrong.</p>
  </li>
  <li><strong>Variance:</strong> This is the error from your model being <em>too complex</em>. A high-variance model ‚Äúoverfits‚Äù the data. It learns the training data perfectly, including all its noise and random quirks. When it sees new data, it performs poorly because the new data doesn‚Äôt have the <em>exact same</em> quirks.</li>
  <li><strong>Analogy:</strong> An archer with an unsteady hand. Their shots are scattered all over the target. On average, they might be centered on the bullseye, but any single shot is unreliable.</li>
</ul>

<h3 id="the-math">The Math</h3>

<p>The Bias-Variance Tradeoff is a way to decompose the total expected error of a model.</p>

<p>Let‚Äôs define our terms:</p>

<ul>
  <li>$y$: The true value we are trying to predict.</li>
  <li>$x$: Our input data.</li>
  <li>$f(x)$: The true, unknown function that perfectly describes the relationship between $x$ and $y$.</li>
  <li>$\hat{f}(x)$: Our machine learning model, which is an <em>estimate</em> of $f(x)$.</li>
  <li>$\epsilon$: Random, unavoidable noise in the data, with a mean of 0 and variance $\sigma^2$. So, $y = f(x) + \epsilon$.</li>
</ul>

<p>The most common way to measure a model‚Äôs error is the <strong>Mean Squared Error (MSE)</strong>. We want to find the <em>expected</em> MSE at a specific data point $x$:</p>

<div class="formula-box">
$$\text{E}\!\left[(y - \hat{f}(x))^2\right]$$
</div>

<p>This single equation can be broken down (decomposed) into three distinct parts:</p>

<div class="formula-box">
$$\text{E}\!\left[(y - \hat{f}(x))^2\right] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$
</div>

<p>This looks complex, so let‚Äôs break down each piece.</p>

<h3 id="1-bias-squared">1. Bias (Squared)</h3>

<ul>
  <li><strong>Equation:</strong></li>
</ul>

<div class="formula-box">
$$\text{Bias}^2 = \left(\text{E}[\hat{f}(x)] - f(x)\right)^2$$
</div>

<ul>
  <li><strong>What it means:</strong>
    <ul>
      <li>$\text{E}[\hat{f}(x)]$ is the <em>average prediction</em> your model would make at point $x$ if you trained it many times on different samples of the data.</li>
      <li>This term measures the difference between your model‚Äôs <em>average prediction</em> and the <em>true value</em> $f(x)$.</li>
      <li><strong>High Bias (Underfitting):</strong> Your model, on average, is simply wrong. It‚Äôs too simple to capture the true pattern. Think of a linear model trying to fit a sine wave.</li>
    </ul>
  </li>
</ul>

<h3 id="2-variance">2. Variance</h3>

<ul>
  <li><strong>Equation:</strong></li>
</ul>

<div class="formula-box">
$$\text{Variance} = \text{E}\!\left[(\hat{f}(x) - \text{E}[\hat{f}(x)])^2\right]$$
</div>

<ul>
  <li><strong>What it means:</strong>
    <ul>
      <li>This measures how much your model‚Äôs specific prediction $\hat{f}(x)$ <em>scatters</em> or <em>varies</em> around its own average prediction $\text{E}[\hat{f}(x)]$.</li>
      <li><strong>High Variance (Overfitting):</strong> Your model is unstable. It‚Äôs so complex that it changes drastically based on the <em>specific</em> training data it sees, learning the noise instead of the signal.</li>
    </ul>
  </li>
</ul>

<h3 id="3-irreducible-error">3. Irreducible Error</h3>

<ul>
  <li><strong>Equation:</strong></li>
</ul>

<div class="formula-box">
$$\text{Irreducible Error} = \sigma^2$$
</div>

<ul>
  <li><strong>What it means:</strong>
    <ul>
      <li>This is the variance of the noise $\epsilon$ that‚Äôs inherent in the data itself.</li>
      <li>This error is unavoidable. No model, no matter how perfect, can get rid of it. It‚Äôs the baseline level of ‚Äúfuzziness‚Äù in the problem.</li>
    </ul>
  </li>
</ul>

<p>So, the full equation is:</p>

<div class="formula-box">
$$\text{E}\!\left[(y - \hat{f}(x))^2\right] = \left(\text{E}[\hat{f}(x)] - f(x)\right)^2 + \text{E}\!\left[(\hat{f}(x) - \text{E}[\hat{f}(x)])^2\right] + \sigma^2$$
</div>

<p>The ‚Äútradeoff‚Äù is that trying to decrease $\text{Bias}^2$ (e.g., by making your model more complex) often <em>increases</em> $\text{Variance}$, and vice-versa.</p>

<p><strong>The Tradeoff:</strong></p>

<ul>
  <li>A simple model (like linear regression) has <strong>high bias</strong> (it can‚Äôt capture complex patterns) and <strong>low variance</strong> (it gives consistent results).</li>
  <li>A complex model (like a very deep decision tree) has <strong>low bias</strong> (it can fit <em>any</em> pattern) and <strong>high variance</strong> (it‚Äôs unstable and changes wildly with new data).</li>
</ul>

<p>The goal is to find a model with <strong>low bias</strong> and <strong>low variance</strong>. This is where ensemble methods come in.</p>

<h2 id="2-the-building-block-decision-trees">2. The Building Block: Decision Trees</h2>

<p>A <strong>Decision Tree</strong> makes predictions by asking a series of ‚Äúif/then‚Äù questions based on the features.</p>

<ul>
  <li><strong>How it works:</strong> It splits the data at each ‚Äúnode‚Äù to create ‚Äúbranches.‚Äù For example: ‚ÄúIs <code class="language-plaintext highlighter-rouge">age</code> &gt; 30?‚Äù If yes, go left; if no, go right. Then, ‚ÄúIs <code class="language-plaintext highlighter-rouge">salary</code> &gt; $50k?‚Äù and so on, until it reaches a ‚Äúleaf‚Äù node with a final prediction (e.g., ‚ÄúWill Buy‚Äù).</li>
  <li><strong>The Problem:</strong> A single, fully-grown decision tree is a classic <strong>high-variance, low-bias</strong> model. It will perfectly memorize the training data (overfit) and won‚Äôt generalize well to new data.</li>
</ul>

<h2 id="3-the-ensemble-methods-bagging--boosting">3. The Ensemble Methods: Bagging &amp; Boosting</h2>

<p>Instead of relying on one ‚Äúunstable‚Äù model (like a deep decision tree), we can combine <em>many</em> models. This is called an <strong>ensemble</strong>. Bagging and Boosting are the two main ways to do this.</p>

<h3 id="bagging-eg-random-forest">Bagging (e.g., Random Forest)</h3>

<p><strong>Bagging</strong> (short for <strong>B</strong>ootstrap <strong>Agg</strong>regating) is a technique that <em>attacks the high-variance problem</em>.</p>

<ol>
  <li><strong>Bootstrap:</strong> Create many (e.g., 500) different random ‚Äúbags‚Äù (subsets) of your training data. The data is sampled <em>with replacement</em>, so some data points may appear multiple times in one bag, and others not at all.</li>
  <li><strong>Aggregate:</strong> Train one <em>deep, high-variance</em> decision tree on each bag of data.</li>
  <li><strong>Vote:</strong> To make a new prediction, let all 500 trees ‚Äúvote.‚Äù The majority vote (for classification) or the average (for regression) is the final answer.</li>
</ol>

<p><strong>Why it works:</strong> You have 500 different ‚Äúoverfitted‚Äù models. Each one is ‚Äúunstable‚Äù and wrong in its own unique way. By averaging their predictions, the errors cancel out, resulting in a single, stable, and powerful model with <strong>low variance</strong>.</p>

<div class="callout callout-key">
  <p><strong>The most famous example is the Random Forest.</strong> It‚Äôs a Bagging model using Decision Trees.</p>
</div>

<h3 id="boosting">Boosting</h3>

<p><strong>Boosting</strong> is a technique that <em>attacks the high-bias problem</em>. It builds a model <em>sequentially</em>, where each new model <em>learns from the mistakes</em> of the previous ones.</p>

<ol>
  <li><strong>Start:</strong> Train a very simple, ‚Äúweak‚Äù model (e.g., a tiny decision tree with only one split, called a ‚Äústump‚Äù). This model is high-bias and only slightly better than random guessing.</li>
  <li><strong>Learn from Mistakes:</strong> Identify all the data points the first model got <em>wrong</em>.</li>
  <li><strong>Build Model 2:</strong> Train a <em>new</em> weak model, but this time, tell it to pay <em>special attention</em> to the data points the first model failed on (by giving them more ‚Äúweight‚Äù).</li>
  <li><strong>Repeat:</strong> Repeat this process 100s or 1000s of times. Each new model is an ‚Äúexpert‚Äù on the data that the <em>entire</em> ensemble (all previous models) still gets wrong.</li>
  <li><strong>Final Prediction:</strong> The final prediction is a weighted sum of all the weak models.</li>
</ol>

<p><strong>Why it works:</strong> You start with a high-bias model and iteratively ‚Äúboost‚Äù its performance by adding new models that fix its errors. The result is a single, strong, <strong>low-bias</strong> model.</p>

<h2 id="4-the-advanced-algorithms">4. The Advanced Algorithms</h2>

<p>These are specific, highly optimized implementations of boosting.</p>

<h3 id="gbm-gradient-boosting-machine">GBM (Gradient Boosting Machine)</h3>

<p>This is the generalized, ‚Äúmathematical‚Äù version of Boosting. Instead of just ‚Äúre-weighting‚Äù the mistakes, it uses a more powerful technique.</p>

<ul>
  <li><strong>How it works:</strong> After the first model makes its predictions, the GBM calculates the ‚Äúresidual errors‚Äù (the difference between the true value and the predicted value).</li>
  <li>The <em>second</em> model is then trained to <strong>predict those errors</strong>.</li>
  <li>You add this ‚Äúerror-correcting‚Äù prediction to the first model‚Äôs prediction.</li>
  <li>The <em>third</em> model is trained to predict the <em>remaining</em> errors, and so on.</li>
  <li>It uses <strong>gradient descent</strong> (a common optimization algorithm) to find the best way to add each new model to minimize the overall error.</li>
</ul>

<h3 id="xgboost-extreme-gradient-boosting">XGBoost (Extreme Gradient Boosting)</h3>

<p><strong>XGBoost</strong> is, for all practical purposes, a <em>better, faster, and more robust</em> version of GBM. It‚Äôs essentially GBM ‚Äúon steroids.‚Äù</p>

<p>It was created to be highly efficient and to win machine learning competitions. It improves on GBM in several key ways:</p>

<ol>
  <li><strong>Regularization (L1 &amp; L2):</strong> It has built-in penalties for model complexity, which <strong>fights overfitting</strong> (a common problem for boosting).</li>
  <li><strong>Parallel Processing:</strong> It‚Äôs designed from the ground up to be ‚Äúparallelized,‚Äù meaning it can use all the cores of your CPU to build trees much faster.</li>
  <li><strong>Smarter Tree Pruning:</strong> It has a more advanced way of ‚Äúpruning‚Äù (simplifying) its trees as it builds them.</li>
  <li><strong>Handles Missing Data:</strong> It has a built-in, very clever way to handle <code class="language-plaintext highlighter-rouge">NaN</code> (missing) values in your data automatically.</li>
</ol>

  </article>

  
  
  
  
  <nav class="post-nav">
    
    <div class="post-nav-item prev">
      <span class="nav-label">&larr; Previous</span>
      <a href="/blog/2026/quantization-cnns-fp32-to-int8/">Quantization in CNNs: From FP32 Training to INT8 Deployment</a>
    </div>
    
    
  </nav>
  

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2026 Monish  Keswani.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    
    Last updated: February 16, 2026.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>

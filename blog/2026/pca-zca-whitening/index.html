<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Monish  Keswani


  | PCA and ZCA Whitening: A Comprehensive Study Guide

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->

<meta property="og:site_name" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="https://archmaester.github.io/blog/2026/pca-zca-whitening/" />
<meta property="og:description" content="PCA and ZCA Whitening: A Comprehensive Study Guide" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí°</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2026/pca-zca-whitening/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>







<!-- GoatCounter Analytics -->
<script data-goatcounter="https://archmaester.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://archmaester.github.io/">
       <span class="font-weight-bold">Monish</span>   Keswani
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/contact/">
                contact
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/cv/">
                cv
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <a href="/blog/" class="back-to-blog">&larr; Back to blog</a>

  <header class="post-header">
    <h1 class="post-title">PCA and ZCA Whitening: A Comprehensive Study Guide</h1>
    
    
    
    <p class="post-meta">
      February 14, 2026
      
      <span class="reading-time">&middot; 8 min read</span>
    </p>
    
    <p class="post-description-text">Understanding whitening transforms ‚Äî from eigenvalue decomposition to PCA and ZCA whitening ‚Äî with clear math and intuition.</p>
    
    
    <div class="post-tags">
      
      <span class="tag-badge">machine-learning</span>
      
      <span class="tag-badge">linear-algebra</span>
      
      <span class="tag-badge">preprocessing</span>
      
    </div>
    
  </header>

  
  
  
  
  
  
  <nav class="toc-wrapper">
    <div class="toc-title">Table of Contents</div>
    <ul>
      
        
        
          <li><a href="#1-introduction-to-whitening">1. Introduction to Whitening</a></li>
        
      
        
        
          <li><a href="#2-the-core-math-eigenvalue-decomposition-evd">2. The Core Math: Eigenvalue Decomposition (EVD)</a></li>
        
      
        
        
          <li><a href="#3-the-pca-whitening-process">3. The PCA Whitening Process</a></li>
        
      
        
        
          <li><a href="#4-zca-whitening-zero-phase-component-analysis">4. ZCA Whitening (Zero-Phase Component Analysis)</a></li>
        
      
        
        
          <li><a href="#5-summary-table">5. Summary Table</a></li>
        
      
        
        
          <li><a href="#6-frequently-asked-questions">6. Frequently Asked Questions</a></li>
        
      
    </ul>
  </nav>
  

  <article class="post-content">
    <h2 id="1-introduction-to-whitening">1. Introduction to Whitening</h2>

<p>Whitening (or sphering) is a preprocessing transformation that converts a vector of random variables with a known covariance matrix into a new vector whose covariance is the <strong>Identity Matrix</strong> ($I$).</p>

<div class="callout callout-key">
  <p><strong>Two goals of whitening:</strong></p>
  <ul>
    <li><strong>Decorrelation</strong> ‚Äî all features become linearly independent.</li>
    <li><strong>Unit Variance</strong> ‚Äî every feature is scaled to have a variance of 1.</li>
  </ul>
</div>

<p><img src="/assets/img/pca-whitening-diagram.svg" alt="The Whitening Pipeline: from correlated data through rotation and scaling to spherical (whitened) data" style="width:100%; max-width:900px; margin: 1.5em auto; display:block;" /></p>

<h2 id="2-the-core-math-eigenvalue-decomposition-evd">2. The Core Math: Eigenvalue Decomposition (EVD)</h2>

<p>The foundation of whitening is the decomposition of the Covariance Matrix ($\Sigma$):</p>

<div class="formula-box">
$$\Sigma = U \Lambda U^{\top}$$
</div>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Symbol</th>
      <th style="text-align: left">Meaning</th>
      <th style="text-align: left">Geometric Role</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">$U$</td>
      <td style="text-align: left">Eigenvector matrix</td>
      <td style="text-align: left">Rotation</td>
    </tr>
    <tr>
      <td style="text-align: left">$\Lambda$</td>
      <td style="text-align: left">Diagonal eigenvalue matrix</td>
      <td style="text-align: left">Variance along principal axes</td>
    </tr>
    <tr>
      <td style="text-align: left">$U^{\top}$</td>
      <td style="text-align: left">Transpose of $U$</td>
      <td style="text-align: left">Inverse rotation</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Think of it as: <strong>rotate, scale, rotate back</strong>. The covariance matrix encodes both the shape and the orientation of your data cloud.</p>
</blockquote>

<h2 id="3-the-pca-whitening-process">3. The PCA Whitening Process</h2>

<p>PCA Whitening rotates the data into the principal component space and then rescales it.</p>

<div class="formula-box">
$$X_{PCA\text{-}white} = \Lambda^{-1/2} U^{\top} X_{centered}$$
</div>

<h3 id="steps-for-pca-whitening">Steps for PCA Whitening</h3>

<p>Given a dataset $X$ (where each column is a feature/dimension and each row is a sample), here are the steps to compute the PCA whitened data $X_{PCA\text{-}white}$:</p>

<h4 id="step-1-zero-mean-the-data">Step 1: Zero-Mean the Data</h4>

<p>Subtract the mean of each feature (column) to center the data around the origin.</p>

\[X_{centered} = X - \mu\]

<p>where $\mu$ is the mean vector of the features in $X$.</p>

<h4 id="step-2-compute-the-covariance-matrix-sigma">Step 2: Compute the Covariance Matrix ($\Sigma$)</h4>

<p>Calculate the empirical covariance matrix of the centered data:</p>

\[\Sigma = \frac{1}{N} X_{centered}^{\top} X_{centered}\]

<p>where $N$ is the number of samples (rows).</p>

<h4 id="step-3-eigendecomposition-of-sigma">Step 3: Eigendecomposition of $\Sigma$</h4>

<p>Compute the eigendecomposition of the symmetric covariance matrix $\Sigma$:</p>

\[\Sigma = U \Lambda U^{\top}\]

<ul>
  <li>$U$: The orthogonal matrix whose columns are the <strong>eigenvectors</strong> (principal components) of $\Sigma$. This matrix represents the rotation.</li>
  <li>$\Lambda$: A diagonal matrix whose diagonal entries are the <strong>eigenvalues</strong> ($\lambda_i$) of $\Sigma$. These values represent the variance along each principal component direction.</li>
</ul>

<h4 id="step-4-compute-the-pca-whitening-matrix-w_pca">Step 4: Compute the PCA Whitening Matrix ($W_{PCA}$)</h4>

<p>The whitening transformation matrix is formed by:</p>

\[W_{PCA} = \Lambda^{-1/2} U^{\top}\]

<p>where $\Lambda^{-1/2}$ is a diagonal matrix containing the reciprocal of the square root of each eigenvalue (i.e., $\frac{1}{\sqrt{\lambda_i}}$).</p>

<div class="callout callout-warn">
  <p><strong>Numerical stability:</strong> A small constant $\epsilon$ is often added to the eigenvalues before taking the reciprocal square root ‚Äî $\frac{1}{\sqrt{\lambda_i + \epsilon}}$ ‚Äî to prevent division by zero when an eigenvalue is close to zero.</p>
</div>

<h4 id="step-5-apply-the-transformation">Step 5: Apply the Transformation</h4>

<p>Apply the whitening matrix to the centered data to get the PCA-whitened data:</p>

\[X_{PCA\text{-}white} = W_{PCA} \cdot X_{centered} = \Lambda^{-1/2} U^{\top} X_{centered}\]

<h3 id="why-the-scaling-factor-lambda-12">Why the scaling factor $\Lambda^{-1/2}$?</h3>

<div class="callout callout-tip">
  <ol>
    <li>After rotation ($U^{\top} X$), the variance along each axis is given by $\Lambda$.</li>
    <li>To normalize each axis to unit variance, we multiply by the inverse square root of the eigenvalues.</li>
    <li>This gives us: $\text{Cov}(\Lambda^{-1/2} U^{\top} X) = \Lambda^{-1/2} \Lambda \Lambda^{-1/2} = I$.</li>
  </ol>
</div>

<h2 id="4-zca-whitening-zero-phase-component-analysis">4. ZCA Whitening (Zero-Phase Component Analysis)</h2>

<p>ZCA goes one step further by rotating the data <strong>back</strong> to its original orientation after whitening.</p>

<div class="formula-box">
$$X_{ZCA\text{-}white} = U \Lambda^{-1/2} U^{\top} X_{centered}$$
</div>

<blockquote>
  <p><strong>Why ZCA?</strong> PCA whitening ‚Äúscrambles‚Äù the original feature space into abstract components. ZCA whitens the data while staying as close as possible to the original representation ‚Äî making it ideal for images.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Property</th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Identity Covariance</strong></td>
      <td style="text-align: left">Like PCA whitening, the final covariance is $I$.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Minimal Distortion</strong></td>
      <td style="text-align: left">It stays as close to the original data as possible.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Structure Preservation</strong></td>
      <td style="text-align: left">Excellent for images ‚Äî doesn‚Äôt scramble pixels into abstract components.</td>
    </tr>
  </tbody>
</table>

<h2 id="5-summary-table">5. Summary Table</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Operation</th>
      <th style="text-align: left">Formula</th>
      <th style="text-align: left">Effect</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Centering</strong></td>
      <td style="text-align: left">$X - \mu$</td>
      <td style="text-align: left">Originates data at zero.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Rotation</strong></td>
      <td style="text-align: left">$U^{\top} X$</td>
      <td style="text-align: left">Decorrelates data (PCA Scores).</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>PCA Whitening</strong></td>
      <td style="text-align: left">$\Lambda^{-1/2} U^{\top} X$</td>
      <td style="text-align: left">Decorrelates + Unit Variance.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>ZCA Whitening</strong></td>
      <td style="text-align: left">$U \Lambda^{-1/2} U^{\top} X$</td>
      <td style="text-align: left">Decorrelates + Unit Variance + Original Orientation.</td>
    </tr>
  </tbody>
</table>

<h2 id="6-frequently-asked-questions">6. Frequently Asked Questions</h2>

<details>
  <summary><strong>Q1: Why are we rescaling in PCA Whitening?</strong></summary>

  <p>The rescaling step is essential to achieve the second part of the ‚Äúwhitening‚Äù goal: <strong>unit variance</strong>.</p>

  <p>The full PCA whitening process involves two main linear transformation steps:</p>

  <ol>
    <li><strong>Rotation (Decorrelation):</strong> The data is rotated into the principal component basis. This step uses the matrix of eigenvectors ($U$) to align the data with its principal axes, which <strong>removes all linear correlation</strong> between the new features (components). The covariance matrix of the rotated data, $U^{\top} X$, is now a <strong>diagonal matrix</strong> $\Lambda$, where the diagonal elements ($\lambda_i$) are the eigenvalues (variances) of the data along each component axis.</li>
    <li><strong>Rescaling (Normalization):</strong> The data is rescaled by dividing each component by the square root of its own variance (the corresponding eigenvalue).</li>
  </ol>

  <p>This division ensures that the variance of every component is equal to $1$. The transformation matrix includes $\Lambda^{-1/2}$, which is the diagonal matrix of these $\frac{1}{\sqrt{\lambda_i}}$ factors.</p>

  <p><strong>Purpose of Rescaling:</strong></p>

  <ul>
    <li><strong>Achieve Identity Covariance:</strong> The ultimate goal of whitening is to transform the data so that its final covariance matrix is the <strong>Identity Matrix</strong> ($I$). The rotation step gives you a diagonal covariance matrix ($\Lambda$), but the diagonal entries are the original variances ($\lambda_i$). The rescaling step changes these entries from $\lambda_i$ to $1$. The final covariance matrix of the whitened data, $\text{Cov}(X_{white})$, is $I$.</li>
    <li><strong>Equalize Feature Importance:</strong> By ensuring all features have unit variance, the rescaling prevents features with large natural scales (large $\lambda_i$) from numerically dominating the downstream machine learning algorithm. It makes all features equally important in terms of their variance, which can stabilize training and improve the performance of models that assume spherical data (like certain clustering or neural network algorithms).</li>
  </ul>

</details>

<details>
  <summary><strong>Q2: Can we do dimensionality reduction in whitening?</strong></summary>

  <p><strong>Yes, you can, and in practice, you often do.</strong></p>

  <p>PCA whitening inherently contains the mechanism for dimensionality reduction: <strong>PCA (Principal Component Analysis)</strong>.</p>

  <p><strong>How Dimensionality Reduction is Performed:</strong></p>

  <p>The eigenvalues ($\lambda_i$) computed during the PCA step represent the amount of variance captured by their corresponding principal components.</p>

  <ol>
    <li><strong>Identify Informative Components:</strong> The eigenvalues are typically sorted in descending order. You choose to keep only the top $k$ principal components that capture a sufficient amount of the total variance (e.g., 95% or 99%).</li>
    <li><strong>Truncate the Matrices:</strong> Instead of using the full eigenvector matrix $U$ and eigenvalue matrix $\Lambda$ (both $d \times d$ for $d$ dimensions), you only keep the first $k$ columns of $U$ and the first $k$ diagonal elements of $\Lambda$.</li>
    <li><strong>Apply the Transformation:</strong> You then apply the whitening transformation using these reduced $k$-dimensional matrices.</li>
  </ol>

  <p>The resulting whitened data will have $k$ dimensions (features), where $k &lt; d$, and will still be decorrelated with unit variance.</p>

  <p>Therefore, <strong>PCA whitening</strong> is frequently used as a pre-processing step to both <strong>decorrelate</strong> and <strong>reduce the dimensionality</strong> of the data before feeding it into a subsequent algorithm like a neural network or a classifier.</p>

</details>

<details>
  <summary><strong>Q3: What is the meaning of the decomposition formula $A = U \Lambda U^{\top}$?</strong></summary>

  <p>The expression $A = U \Lambda U^{\top}$ is the most common form of the <strong>Eigenvalue Decomposition (EVD)</strong> for <strong>real symmetric matrices</strong> (like a covariance matrix, $\Sigma$).</p>

  <p>This formula states that a square, symmetric matrix $A$ can be factored into a product of three matrices:</p>

  <ul>
    <li><strong>$U$ (The Eigenvector Matrix):</strong>
      <ul>
        <li>This is an <strong>orthogonal matrix</strong> whose columns are the <strong>eigenvectors</strong> of $A$.</li>
        <li>Since it is orthogonal, its transpose is its inverse: $U^{\top} = U^{-1}$. This is why the decomposition is often written as $A = P D P^{-1}$ for non-symmetric matrices, but simplifies to $A = U \Lambda U^{\top}$ for symmetric matrices.</li>
        <li><strong>Geometric Meaning:</strong> The matrix $U$ represents a <strong>rotation</strong> (or reflection) that aligns the coordinate axes with the principal axes of the data defined by $A$.</li>
      </ul>
    </li>
    <li><strong>$\Lambda$ (Lambda - The Eigenvalue Matrix):</strong>
      <ul>
        <li>This is a <strong>diagonal matrix</strong> whose diagonal entries are the <strong>eigenvalues</strong> ($\lambda_i$) of $A$.</li>
        <li><strong>Geometric Meaning:</strong> The eigenvalues represent the <strong>scaling factors</strong> (the variance) along each new coordinate axis defined by the eigenvectors in $U$.</li>
      </ul>
    </li>
    <li><strong>$U^{\top}$ (U Transpose):</strong>
      <ul>
        <li>This is the transpose of the eigenvector matrix, which is also its inverse ($U^{-1}$).</li>
        <li><strong>Geometric Meaning:</strong> This represents the inverse rotation needed to return to the original coordinate system.</li>
      </ul>
    </li>
  </ul>

</details>

<details>
  <summary><strong>Q4: What is the significance of EVD in data analysis (like PCA)?</strong></summary>

  <p>The decomposition $A = U \Lambda U^{\top}$ is incredibly important because it allows you to interpret the complex action of the matrix $A$ as a simple sequence of geometric transformations:</p>

  <ol>
    <li><strong>$U^{\top}$:</strong> Rotate the vector $x$ into the basis of the eigenvectors.</li>
    <li><strong>$\Lambda$:</strong> Scale the rotated vector independently along each new axis (by the eigenvalues).</li>
    <li><strong>$U$:</strong> Rotate the result back to the original coordinate system.</li>
  </ol>

  <p>In the context of <strong>Principal Component Analysis (PCA)</strong>, where $A$ is the <strong>covariance matrix</strong> ($\Sigma$), this decomposition is the foundation:</p>

  <ol>
    <li>The columns of $U$ are the <strong>Principal Components (PCs)</strong>.</li>
    <li>The diagonal entries of $\Lambda$ are the <strong>variances</strong> along those PCs.</li>
  </ol>

</details>

<details>
  <summary><strong>Q5: Why is centering (zero-meaning) required in PCA?</strong></summary>

  <p>Centering is the process of subtracting the mean of each feature from the dataset. In PCA, this is a non-negotiable preprocessing step because it ensures that the algorithm focuses on the <strong>internal structure (variance)</strong> of the data rather than its absolute position in space.</p>

  <h4 id="the-mathematical-requirement-of-covariance">1. The Mathematical Requirement of Covariance</h4>

  <p>PCA is based on the <strong>Covariance Matrix</strong> ($\Sigma$). The statistical formula for covariance between two variables $X$ and $Y$ is:</p>

\[\text{Cov}(X, Y) = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})(y_i - \bar{y})\]

  <p>Notice that the formula explicitly subtracts the means ($\bar{x}$ and $\bar{y}$). In matrix notation, we often simplify the covariance calculation to:</p>

\[\Sigma = \frac{1}{N} X^{\top} X\]

  <p>This simplified matrix multiplication <strong>only works if the data is already centered</strong> (where $\bar{x} = 0$). If you skip centering, $X^{\top} X$ calculates the ‚Äúraw moments‚Äù (distance from the origin) rather than the variance (distance from the mean), leading to an incorrect covariance matrix.</p>

  <h4 id="geometric-alignment-of-principal-components">2. Geometric Alignment of Principal Components</h4>

  <p>PCA seeks to find the directions (eigenvectors) of maximum spread. These vectors must originate from the origin $(0, 0)$.</p>

  <ul>
    <li><strong>With Centering:</strong> The data cloud is shifted so its center of mass is at $(0, 0)$. The first principal component (PC1) then aligns perfectly with the direction of the greatest spread within the cluster.</li>
    <li><strong>Without Centering:</strong> If the data cluster is far from the origin, the first principal component will likely point from the origin directly toward the ‚Äúcenter‚Äù of the data cluster. Instead of capturing the <em>variance</em> (the shape of the cloud), it captures the <em>offset</em> (the position of the cloud).</li>
  </ul>

  <h4 id="orthogonality-and-basis-transformation">3. Orthogonality and Basis Transformation</h4>

  <p>PCA creates a new coordinate system. For the new axes to be valid rotations of the original axes, they must share the same origin. If the data isn‚Äôt centered, the ‚Äúrotation‚Äù performed by the eigenvector matrix $U$ will be performed around the coordinate $(0, 0)$ rather than the center of the data, which ‚Äúscrambles‚Äù the relationship between the features and makes the resulting components uninterpretable.</p>

  <h4 id="impact-on-reconstruction">4. Impact on Reconstruction</h4>

  <p>If you use PCA for dimensionality reduction and then try to reconstruct the original data, the reconstruction will be fundamentally broken if you didn‚Äôt center. PCA assumes that the data is a linear combination of components <strong>plus the mean</strong>. If the mean isn‚Äôt handled correctly at the start, you lose the baseline needed to map the compressed data back into its original space.</p>

</details>

  </article>

  
  
  
  
  <nav class="post-nav">
    
    <div class="post-nav-item prev">
      <span class="nav-label">&larr; Previous</span>
      <a href="/blog/2026/depthwise-pointwise-convolutions-edge-ai/">Depthwise and Pointwise Convolutions: A Practical Guide for Edge AI</a>
    </div>
    
    
    <div class="post-nav-item next">
      <span class="nav-label">Next &rarr;</span>
      <a href="/blog/2026/quantization-cnns-fp32-to-int8/">Quantization in CNNs: From FP32 Training to INT8 Deployment</a>
    </div>
    
  </nav>
  

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2026 Monish  Keswani.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    
    Last updated: February 16, 2026.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
